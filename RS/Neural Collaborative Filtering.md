



# **Neural Collaborative Filtering**

*WWW 2017,* April 3–7, 2017, Perth, Australia



通过将内积运算替换为可以从数据中学习任意函数的神经体系结构，本文提出了一个名为NCF（Neural network based Collaborative Filtering）的通用框架。

主要贡献：

1.提出神经网络框架建模user和item的潜在特质，设计了通用的NCF框架用于协同过滤

2.MF可理解为NCF的特例，利用多层感知器可以赋予NCF高水平的非线性建模

## **PRELIMINARIES**

用户-item为1表示点击，为0表示不点击，但点击不代表用户喜欢item，不点击不代表用户不喜欢item。

虽然观察到的交互数据反映了用户对物品的兴趣，但未观察到的交互数据可能只是缺少数据，而且自然缺乏负面反馈。

隐式反馈的推荐问题被表述为估计用户-物品交互矩阵Y中未观察物品的分数的问题，该分数用于对物品进行排序。基于模型的方法假设可以生成数据 (或由底层模型描述)：

![截屏2021-05-23 下午4.18.47](https://i.loli.net/2021/05/23/F4BbueNR1DPxK3a.png)

表示用户和物品的预测得分

为了估计参数，现有的方法通常遵循优化目标函数的机器学习范式。文献中最常用的目标函数有两种——pointwise loss和pairwise loss。

pointwise loss通过最小化预测得分和目标的分的平方损失，为了处理没有负面数据的情况，他们要么将所有未观察到的条目视为负面反馈，要么从未观察到的条目中采样负面实例。

pairwise learning的思想观察到的条目应该比未观察到的条目排名高，最大化观察到的值和未观察到的值之间的margin。

NCF支持二者

##  **Matrix Factorization**

MF将每个用户和物品与潜在特征的实值向量相关联

设$p_u$和$q_i$分别表示用户u和物品i的隐向量，

![截屏2021-05-24 下午4.19.32](https://i.loli.net/2021/05/24/F3gtZvoiGpJkL7s.png)

K表示隐空间的维度

MF模型对用户和物品的潜在因子的双向交互进行建模，假设隐空间的每个维度都是相互独立的，并以相同的权重将它们线性组合。因此，MF可视为潜在因子的线性模型。

![截屏2021-05-24 下午4.22.30](https://i.loli.net/2021/05/24/nhZrNJfPUFH5zmW.png)

图1说明了内积函数如何限制MF的表现力。为了更好地理解这个示例，有两个设置需要事先明确说明。第一，由于MF将用户和项目映射到相同的隐空间，所以两个用户之间的相似度也可以用一个内积（隐向量夹角的余弦值）来衡量。第二，在不失一般性的前提下，使用Jaccard系数作为MF需要重新获得的两个用户之间的相似度。

Jaccard系数：交集与并集的比例

![截屏2021-05-24 下午4.26.10](https://i.loli.net/2021/05/24/BNnfsa7wqRxbMuO.png)

i和j都表示用户

由前三行得到：$s_{23}(0.66)>s_{12}(0.5)>s_{13}(0.4)$,对应隐空间的表示关系如图1（b）p1、p2、p3

考虑新用户u4，可以得到$s_{41}(0.)>s_{43}(0.4)>s_{42(0.2)$，即u4最接近u1.

当在隐空间将u4对应的p4放在离u1对应的p1最近的位置时，这会导致p4和p2更接近而不是p3，导致巨大的ranking loss

这个例子说明了在低维隐空间中，使用一个简单而固定的内积来估计复杂的用户-项目交互可能会导致MF的局限性。我们注意到，解决这一问题的一个办法是利用大量的潜在因素。然而，这可能会对模型的泛化造成不利影响(例如，数据的过度拟合)，特别是在稀疏环境中。在本文的工作中，我们通过使用深度神经网络从数据中进行学习构造相互作用函数来解决这个问题。

【此处想到的是只能将隐空间扩大，使得到某个维度时可以使原来的排名合理】

##  **NEURAL COLLABORATIVE FILTERING**

首先提出通用的NCF框架，详细的解释了使用概率模型强调隐式数据的二值属性的NCF如何进行学习。

然后证明了在NCF下MF可以被表达和推广。

为了探索深度神经网络的协同过滤，提出了NCF的实例化，使用多层感知器(MLP)学习用户-项目的交互函数。

在NCF框架下结合了MF和MLP；在对用户-物品潜在结构的建模过程中它综合了MF的线性优点和MLP的非线性优点，让模型的表达能力更强。

### **General Framework**

![截屏2021-05-24 下午4.36.01](https://i.loli.net/2021/05/24/MAj4vm5OkS8raJo.png)



使用user 身份和item作为输入特征，转化为one-hot向量。

Embedding Layer：将稀疏表示投影到稠密向量上。得到的用户(项目)Embedding可以看作是用户(项目)在隐因子模型上下文中的隐向量。

Neural CF Layers：用户Embedding和项目Embedding分别注入多层神经网络结构，得到预测分数。可以自定义NeuralCF中的每一层，以发现用户-物品交互的某些潜在结构。

训练是通过最小化预测分数和目标分数之间的pointwise loss

![截屏2021-05-24 下午4.43.45](https://i.loli.net/2021/05/24/kSQM4CHsbxvF93V.png)

函数f可以定义为多层神经网络，可以被表述为：

![截屏2021-05-24 下午5.44.51](/Users/mac/Library/Application Support/typora-user-images/截屏2021-05-24 下午5.44.51.png)

####  *Learning NCF*

一般使用平方差

![截屏2021-05-24 下午5.46.20](/Users/mac/Library/Application Support/typora-user-images/截屏2021-05-24 下午5.46.20.png)

y表示了在Y中观察到的相互作用的集合，y-表示负向实例的集合。

然平方损失可以通过假设观测结果是由高斯分布产生来解释，但我们指出它可能与隐式数据不符。这是因为对于隐式数据，目标值是一个二值化的1或0，表示u是否与i交互。

考虑到隐式反馈的一类性质，我们可以将 目标得分的值看作一个标签—1表示物品i与用户u相关，否则为0。

预测得分表示用户和item的相似性，因此需要概率函数将预测得分限制在【0，1】作为激活函数。

定义似然函数为：

![截屏2021-05-24 下午5.50.18](https://i.loli.net/2021/05/24/s9El6MaIitUQ3cq.png)

取负数：

![截屏2021-05-24 下午5.51.20](https://i.loli.net/2021/05/24/l7jyzs645oPDFBR.png)

得到NCF的目标函数

通过对NCF的概率处理，我们将隐式反馈的推荐作为一个二分类问题来解决。对于负实例y-,在每次迭代中我们从未观察到的交互进行一致的采样并且控制采样率，即观察到的相互作用的数量。

### **Generalized Matrix Factorization (GMF)**

由于对输入层的用户(项目)ID进行one-hot编码，得到的Embedding向量可视为用户(项目)的隐向量。

假设用户的隐向量为 $p_u = P^Tv^U_u$，item的隐向量为 $q_i = Q^Tv^I_i$

CF的第一层映射函数为：
![截屏2021-05-24 下午5.56.20](https://i.loli.net/2021/05/24/Jslb3hU9IYrgv78.png)

然后将向量投影到输出层：
![截屏2021-05-24 下午5.56.52](https://i.loli.net/2021/05/24/oR5ibLU2NpQqfKC.png)

将$a_{out}$函数使用恒等函数，h为1，就可将NCF恢复为MF模型。在NCF框架下，MF可以很容易地推广和扩展。

此处使用sigmoid函数，通过损失学习h

###  **Multi-Layer Perceptron (MLP)**

由于NCF采用两种路径来对用户和项目进行建模，所以将这两种路径的特征串联起来是很直观的。然而，简单的向量连接并不能解释用户和物品的潜在特征之间的任何交互，这对于协同过滤进行建模效果是不够的。为了解决这个问题，我们建议在连接的向量上添加隐藏层，使用一个标准的MLP来学习用户和物品潜在特征之间的交互。

![截屏2021-05-24 下午6.01.10](https://i.loli.net/2021/05/24/yL7KFaj1GCuWwMT.png)

对于MLP层的激活函数，可以自由选择sigmoid、tanh和ReLU等函数。实验结果表明，ReLU的性能略好于tanh，而tanh的性能又明显好于sigmoid。

对于网络结构的设计，通常的解决方案是采用塔状结构，底层最宽，每一层的神经元数量逐级变少（如图2）。本文根据经验实现了塔式结构，将每一层的尺寸减半。

### **Fusion of GMF and MLP**

两个NCF实例：GMF采用线性核函数对潜在特征交互进行建模，MLP采用非线性核函数从数据中学习交互函数。

在NCF框架下融合GMF和MLP，使得它们可以相互增强，从而更好地对复杂的用户-物品矩阵迭代交互进行建模。

一个简单的解决方案是让GMF和MLP共享相同的Embedding层，然后合并它们的交互函数的输出。具体来说，是将GMF与单层结构相结合的模型MLP表示为

![截屏2021-05-24 下午6.05.16](https://i.loli.net/2021/05/24/mqpW9iRSTgGCZ4B.png)

然而，共享GMF和MLP的Embedding可能会限制融合模型的性能。例如，它意味着GMF和MLP必须使用相同大小的Embedding;对于两种模型的最优嵌入尺寸变化较大的数据集，该方案可能无法获得最优的集成。

因此允许GMF和MLP学习独立的用户Embedding和item的Embedding，并通过连接它们的最后一个隐藏层来组合这两个模型。

![截屏2021-05-25 上午9.12.53](https://i.loli.net/2021/05/25/sRB42pKGIx8rfSn.png)

#### *Pre-training*

使用GMF和MLP的预训练模型初始化NeuMF

首先用随机初始化训练GMF和MLP，直到收敛。然后，我们使用它们的模型参数作为NeuMF参数的相应部分的初始化。唯一的调整是在输出层，我们将两个模型的权值连接在一起

![截屏2021-05-25 上午9.26.58](https://i.loli.net/2021/05/25/H6BtZnbpKzlCuLJ.png)





##  **EXPERIMENTS**

#### 目标

问题1：本文提出的NCF方法是否优于目前最先进的隐式协同过滤方法?

问题2：提出的优化框架(采用负采样的log损失)如何工作于推荐任务?

问题3：更深层次的隐藏单元是否有助于从用户-项目交互数据中学习?

#### **Datasets**

![截屏2021-05-25 上午9.35.35](https://i.loli.net/2021/05/25/sjRp3WG6EgXxb4d.png)

### **Performance**

![截屏2021-05-25 上午9.37.58](/Users/mac/Library/Application Support/typora-user-images/截屏2021-05-25 上午9.37.58.png)



![截屏2021-05-25 上午9.39.35](https://i.loli.net/2021/05/25/tqD6freIiclmRJo.png)

#####  *Utility of Pre-training*

比较了两个版本的NeuMF的性能——有预训练和没有预训练。

![截屏2021-05-25 上午9.40.44](https://i.loli.net/2021/05/25/lNzoyvsZ4MLKmD1.png)

#####  **Log Loss with Negative Sampling (RQ2)**

![截屏2021-05-25 上午9.44.34](https://i.loli.net/2021/05/25/bz8gvJY7h5eqRjE.png)



![截屏2021-05-25 上午9.45.51](https://i.loli.net/2021/05/25/Zey2xaW3udE8CAS.png)

##### **Is Deep Learning Helpful? (RQ3)**

![截屏2021-05-25 上午9.46.33](https://i.loli.net/2021/05/25/7PkuIGTjwov2tMR.png)







# 总结

